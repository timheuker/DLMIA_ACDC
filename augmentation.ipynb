{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of augmentation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2cTZJ_-7ZvlM"},"outputs":[],"source":["!pip install nilearn\n","!pip install monai"]},{"cell_type":"code","source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","data_path = r'/content/drive/MyDrive/Deep Learning Project/training'\n","\n","import os\n","from nibabel.testing import data_path\n","import nilearn.image\n","import numpy as np\n","from nilearn.image import resample_img\n","import nibabel as nib\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","import pandas as pd\n","import monai\n","import torch\n","from tqdm import tqdm\n","import scipy\n","import random\n","import cv2\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f'The used device is {device}')\n","\n","file_path = r'/content/drive/MyDrive/Deep Learning Project/training'\n","patient_files = [name for name in os.listdir(file_path) if os.path.isdir(os.path.join(file_path, name))]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1dflShAZzW5","executionInfo":{"status":"ok","timestamp":1657034549255,"user_tz":-120,"elapsed":29483,"user":{"displayName":"Tim Heuker of Hoek","userId":"08665850295594550490"}},"outputId":"4bb0419d-e594-43ea-d4c1-09b1af69775d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","The used device is cpu\n"]}]},{"cell_type":"code","source":["# Function for zero-padding the images to a certain shape\n","def to_shape(a, shape):\n","    y_, x_ = shape\n","    y, x = a.shape\n","    y_pad = (y_-y)\n","    x_pad = (x_-x)\n","    return np.pad(a,((y_pad//2, y_pad//2 + y_pad%2), \n","                     (x_pad//2, x_pad//2 + x_pad%2)),\n","                  mode = 'constant')\n","\n","# Function for normalizing the histograms of each image    \n","def image_histogram_equalization(image, number_bins=256):\n","    # from http://www.janeriksolem.net/histogram-equalization-with-python-and.html\n","\n","    # get image histogram\n","    image_histogram, bins = np.histogram(image.flatten(), number_bins, density=True)\n","    cdf = image_histogram.cumsum() # cumulative distribution function\n","    cdf = 255 * cdf / cdf[-1] # normalize\n","\n","    # use linear interpolation of cdf to find new pixel values\n","    image_equalized = np.interp(image.flatten(), bins[:-1], cdf)\n","\n","    return image_equalized.reshape(image.shape), cdf\n","\n","# Function for rotating the images\n","def rotate_CV(image, angel , interpolation):\n","\n","    h,w = image.shape[:2]\n","    cX,cY = (w//2,h//2)\n","    M = cv2.getRotationMatrix2D((cX,cY),angel,1)\n","    rotated = cv2.warpAffine(image,M , (w,h),flags=interpolation)\n","    return rotated\n","\n","# Function for scaling the images\n","def zoom_CV(image, scale , interpolation):\n","\n","    h,w = image.shape[:2]\n","    cX,cY = (w//2,h//2)\n","    M = cv2.getRotationMatrix2D((cX,cY),0,scale)\n","    scaled = cv2.warpAffine(image,M , (w,h),flags=interpolation)\n","    return scaled\n","\n","# Function for cropping the images to output_size\n","def crop_img(img_sys, gt_sys, img_dia, gt_dia, output_size):\n","\n","  output_size = output_size + 1\n","  absdiff = abs(img_sys - img_dia)\n","\n","  if absdiff.shape[0] < output_size:\n","    img_sys = to_shape(img_sys,[output_size,absdiff.shape[1]])\n","    gt_sys = to_shape(gt_sys,[output_size,absdiff.shape[1]])\n","    img_dia = to_shape(img_dia,[output_size,absdiff.shape[1]])\n","    gt_dia = to_shape(gt_dia,[output_size,absdiff.shape[1]])\n","\n","  if absdiff.shape[1] < output_size:\n","    img_sys = to_shape(img_sys,[absdiff.shape[0],output_size])\n","    gt_sys = to_shape(gt_sys,[absdiff.shape[0],output_size])\n","    img_dia = to_shape(img_dia,[absdiff.shape[0],output_size])\n","    gt_dia = to_shape(gt_dia,[absdiff.shape[0],output_size])\n","\n","  cord = []\n","  absdiff = abs(img_sys - img_dia)\n","  output_size = output_size - 1\n","\n","  intsum = np.zeros([absdiff.shape[0]-output_size,absdiff.shape[1]-output_size])\n","  intsum[:] = np.nan\n","  for i in range(absdiff.shape[0]-output_size):\n","    for j in range(absdiff.shape[1]-output_size):\n","      intsum[i,j] = sum(sum(absdiff[i:i+output_size,j:j+output_size]))\n","\n","  result = np.where(intsum == np.amax(intsum))\n","  cord = list(zip(result[0], result[1]))\n","\n","  img_sys_cut = img_sys[cord[0][0]:output_size+cord[0][0],cord[0][1]:output_size+cord[0][1]]\n","  gt_sys_cut = gt_sys[cord[0][0]:output_size+cord[0][0],cord[0][1]:output_size+cord[0][1]]\n","  img_dia_cut = img_dia[cord[0][0]:output_size+cord[0][0],cord[0][1]:output_size+cord[0][1]]\n","  gt_dia_cut = gt_dia[cord[0][0]:output_size+cord[0][0],cord[0][1]:output_size+cord[0][1]]\n","\n","  return img_sys_cut, gt_sys_cut, img_dia_cut, gt_dia_cut"],"metadata":{"id":"VsXGbZcrieDR","executionInfo":{"status":"ok","timestamp":1657034810677,"user_tz":-120,"elapsed":215,"user":{"displayName":"Tim Heuker of Hoek","userId":"08665850295594550490"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Remove some patients if the data does not exist or the image is too large for resampling\n","patient_files_true = []\n","\n","for i in range(len(patient_files)):\n","  try:\n","    pt_nr = i\n","    img = nib.load(file_path+'/'+patient_files[pt_nr]+'/'+patient_files[pt_nr]+'_frame01.nii.gz')\n","    ds_img = resample_img(img, target_affine=np.diag([1.25,1.25,1]), interpolation='nearest')\n","    #print(ds_img.shape)\n","    #print(out.shape)\n","    patient_files_true.append(patient_files[pt_nr])\n","\n","  except:\n","    print(\"Removing patient:\", patient_files[pt_nr])\n"],"metadata":{"id":"gdCkgSmKioci","executionInfo":{"status":"ok","timestamp":1657034622552,"user_tz":-120,"elapsed":56428,"user":{"displayName":"Tim Heuker of Hoek","userId":"08665850295594550490"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def augment_images(file_path, patient_files, pt_nr, n_images, V_var):\n","  # Function for data augmentation\n","\n","  # Inputs:\n","  # file_path: path to all patient maps\n","  # patient_files: list of patient files\n","  # pt_nr: idx in patient files\n","  # n_images: number of generated images per slice\n","  # V_var: variance of the V-component in the intensity transformation\n","\n","  # Outputs:\n","  # aug_img_all: augmented images of shape (x_pixel_size,y_pixel_size,n_images,z_slice,frame)\n","  #              where frame=0 is for systole and frame=1 is for diastole\n","  # aug_gt_all: augmentend ground truths of the same shape as aug_img_all\n","\n","  path = os.listdir(file_path+'/'+patient_files_true[pt_nr])\n","\n","  # Find the right files\n","  for i in range(len(path)):\n","    if path[i].endswith('frame01.nii.gz'):\n","      i_dia = i\n","    if path[i].endswith('frame01_gt.nii.gz'):\n","      i_dia_gt = i\n","    if 'frame' in path[i] and not path[i].endswith('_frame01.nii.gz') and not path[i].endswith('_gt.nii.gz'):\n","      i_sys = i\n","    if path[i].endswith('_gt.nii.gz') and not path[i].endswith('frame01_gt.nii.gz'):\n","      i_sys_gt = i\n","      \n","  # Load and resample the systolic and diastolic 3D images to 1,25x1,25x10\n","  img_sys = nib.load(file_path+'/'+patient_files[pt_nr]+'/'+path[i_sys])\n","  img_sys_ds = resample_img(img_sys, target_affine=np.diag([1.25,1.25,1]), interpolation='nearest')\n","  img_dia = nib.load(file_path+'/'+patient_files[pt_nr]+'/'+path[i_dia])\n","  img_dia_ds = resample_img(img_dia, target_affine=np.diag([1.25,1.25,1]), interpolation='nearest')\n","  img_sys_gt = nib.load(file_path+'/'+patient_files[pt_nr]+'/'+path[i_sys_gt])\n","  img_sys_gt_ds = resample_img(img_sys_gt, target_affine=np.diag([1.25,1.25,1]), interpolation='nearest')\n","  img_dia_gt = nib.load(file_path+'/'+patient_files[pt_nr]+'/'+path[i_dia_gt])\n","  img_dia_gt_ds = resample_img(img_dia_gt, target_affine=np.diag([1.25,1.25,1]), interpolation='nearest')\n","\n","  theta_ = np.random.uniform(0, 180, size=n_images-1)\n","  zoom_ = np.random.uniform(1, 1.3, size=n_images-1)\n","\n","  aug_img_all = np.zeros([152,152,n_images,2,img_sys_ds.get_fdata(caching='unchanged').shape[2]])\n","  aug_gt_all = np.zeros([152,152,n_images,2,img_sys_ds.get_fdata(caching='unchanged').shape[2]])\n","  aug_img_all[:] = np.nan\n","  for augment in range(1): \n","\n","    for s in range(img_sys_ds.get_fdata(caching='unchanged').shape[2]):\n","\n","      slice_0 = img_sys_ds.get_fdata(caching='unchanged')[:,:,s]\n","      slice_1 = img_sys_gt_ds.get_fdata(caching='unchanged')[:,:,s]\n","\n","      slice_2 = img_dia_ds.get_fdata(caching='unchanged')[:,:,s]\n","      slice_3 = img_dia_gt_ds.get_fdata(caching='unchanged')[:,:,s]\n","\n","      pixLV0 = np.where(slice_1 == 3)\n","      pixMYO0 = np.where(slice_1 == 2)\n","      pixLV1 = np.where(slice_3 == 3)\n","      pixMYO1 = np.where(slice_3 == 2)\n","\n","      intLV0 = np.zeros(len(pixLV0[0]))\n","      intLV1 = np.zeros(len(pixLV1[0]))\n","      for i in range(len(pixLV0[0])):\n","        intLV0[i] = slice_0[pixLV0[0][i],pixLV0[1][i]]\n","      for i in range(len(pixLV1[0])):\n","        intLV1[i] = slice_2[pixLV1[0][i],pixLV1[1][i]]  \n","\n","      intMYO0 = np.zeros(len(pixMYO0[0]))\n","      intMYO1 = np.zeros(len(pixMYO1[0]))\n","      for i in range(len(pixMYO0[0])):\n","        intMYO0[i] = slice_0[pixMYO0[0][i],pixMYO0[1][i]]\n","      for i in range(len(pixMYO1[0])):\n","        intMYO1[i] = slice_2[pixMYO1[0][i],pixMYO1[1][i]]\n","\n","      bins_ = np.arange(0,400,1)\n","      freqsLV0, _ = np.histogram(intLV0, bins=bins_)\n","      freqsMYO0, _ = np.histogram(intMYO0, bins=bins_)\n","      freqsLV0 = freqsLV0/len(intLV0)\n","      freqsMYO0 = freqsMYO0/len(intMYO0)\n","      freqsLV1, _ = np.histogram(intLV1, bins=bins_)\n","      freqsMYO1, _ = np.histogram(intMYO1, bins=bins_)\n","      freqsLV1 = freqsLV1/len(intLV1)\n","      freqsMYO1 = freqsMYO1/len(intMYO1)\n","\n","      BC0 = sum(np.sqrt(freqsLV0*freqsMYO0)) # Bhattacharyya coefficient (between 0-1)\n","      BC1 = sum(np.sqrt(freqsLV1*freqsMYO1))\n","      DB0 = -np.log(BC0) # Bhattacharyya distance\n","      DB1 = -np.log(BC1)\n","\n","      # Take an arbritary value for DB if pixLV or pixMYO is empty:\n","      if np.isnan(DB0) or DB0 > 3 or DB0 < -3:\n","        DB0 = 1.3\n","      if np.isnan(DB1) or DB1 > 3 or DB1 < -3:\n","        DB1 = 1.3\n","\n","      aug_img = np.zeros([152,152,n_images,2])\n","      aug_gt = np.zeros([152,152,n_images,2])\n","      aug_img[:] = np.nan\n","\n","      # Store original image as k=0\n","      slice_0, _ = image_histogram_equalization(slice_0, number_bins=256)\n","      slice_2, _ = image_histogram_equalization(slice_2, number_bins=256)\n","      slice_0, slice_1, slice_2, slice_3 = crop_img(slice_0, slice_1, slice_2, slice_3, 152)\n","      aug_img[:,:,0,0] = slice_0\n","      aug_gt[:,:,0,0] = slice_1\n","      aug_img[:,:,0,1] = slice_2\n","      aug_gt[:,:,0,1] = slice_3\n","      \n","      for k in range(1,n_images):\n","\n","        slice_0 = img_sys_ds.get_fdata(caching='unchanged')[:,:,s]\n","        slice_1 = img_sys_gt_ds.get_fdata(caching='unchanged')[:,:,s]\n","\n","        slice_2 = img_dia_ds.get_fdata(caching='unchanged')[:,:,s]\n","        slice_3 = img_dia_gt_ds.get_fdata(caching='unchanged')[:,:,s]\n","\n","        # Histogram normalization\n","        slice_0, _ = image_histogram_equalization(slice_0, number_bins=256)\n","        slice_2, _ = image_histogram_equalization(slice_2, number_bins=256)\n","        \n","        # Transform intensity\n","        V = np.random.normal(loc=0.1, scale=V_var)\n","\n","        for i in range(len(pixLV0[0])):\n","          W = np.random.uniform(low=-0.05, high=0.05)\n","          slice_0[pixLV0[0][i],pixLV0[1][i]] = (1-DB0*V + W)*slice_0[pixLV0[0][i],pixLV0[1][i]]\n","        for i in range(len(pixLV1[0])):\n","          W = np.random.uniform(low=-0.05, high=0.05)\n","          slice_2[pixLV1[0][i],pixLV1[1][i]] = (1-DB1*V + W)*slice_2[pixLV1[0][i],pixLV1[1][i]]\n","\n","        for i in range(len(pixMYO0[0])):\n","          W = np.random.uniform(low=-0.05, high=0.05)\n","          slice_0[pixMYO0[0][i],pixMYO0[1][i]] = (1+DB0*V + W)*slice_0[pixMYO0[0][i],pixMYO0[1][i]]\n","        for i in range(len(pixMYO1[0])):\n","          W = np.random.uniform(low=-0.05, high=0.05)\n","          slice_2[pixMYO1[0][i],pixMYO1[1][i]] = (1+DB1*V + W)*slice_2[pixMYO1[0][i],pixMYO1[1][i]]\n","\n","        # Rotate randomly (linear for img and nearest for gt)\n","        theta = theta_[k-1]\n","        slice_0 = rotate_CV(slice_0, theta, cv2.INTER_LINEAR)\n","        slice_1 = rotate_CV(slice_1, theta, cv2.INTER_NEAREST)\n","        slice_2 = rotate_CV(slice_2, theta, cv2.INTER_LINEAR)\n","        slice_3 = rotate_CV(slice_3, theta, cv2.INTER_NEAREST)\n","        \n","        # Scale randomly\n","        zoom = zoom_[k-1]\n","        slice_0 = zoom_CV(slice_0, zoom, cv2.INTER_LINEAR)\n","        slice_1 = zoom_CV(slice_1, zoom, cv2.INTER_NEAREST)\n","        slice_2 = zoom_CV(slice_2, zoom, cv2.INTER_LINEAR)\n","        slice_3 = zoom_CV(slice_3, zoom, cv2.INTER_NEAREST)\n","\n","        # Crop to 152x152  \n","        slice_0, slice_1, slice_2, slice_3 = crop_img(slice_0, slice_1, slice_2, slice_3, 152)\n","\n","        # Store augmented data\n","        aug_img[:,:,k,0] = slice_0\n","        aug_gt[:,:,k,0] = slice_1\n","        aug_img[:,:,k,1] = slice_2\n","        aug_gt[:,:,k,1] = slice_3\n","      ################################ END FOR\n","      aug_img_all[:,:,:,:,s] = aug_img\n","      aug_gt_all[:,:,:,:,s] = aug_gt\n","    ################################## END FOR\n","\n","  return aug_img_all, aug_gt_all"],"metadata":{"id":"Y3JWKx93jCEe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_size = 152\n","count_false = 0\n","false_crops = []\n","failed_files = []\n","count_total = 0\n","n_images = 3 # 3 images in total, so 1 original + 2 augmented\n","V_var = 0.02\n","\n","for pt_nr in range(len(patient_files_true)):\n","  pt = patient_files_true[pt_nr][7:] # get patient number\n","\n","  x,y = augment_images(file_path, patient_files_true, pt_nr, n_images, V_var)\n","  augmented = True\n","\n","  if augmented:\n","    for n in range(x.shape[2]):\n","      for f in range(x.shape[3]):\n","        for z in range(x.shape[4]):\n","          try:\n","            x_cut = x[:,:,n,f,z]\n","            y_cut = y[:,:,n,f,z]\n","            np.save(f'/content/drive/MyDrive/Deep Learning Project/im_aug_REAL2/im_p{pt}_z{z}_f{f}_n{n}.npy',x_cut)\n","            np.save(f'/content/drive/MyDrive/Deep Learning Project/gt_aug_REAL2/gt_p{pt}_z{z}_f{f}_n{n}.npy',y_cut)\n","\n","            # Keep track of how many falsely cropped images there are\n","            count_total = count_total + 1\n","            if sum(y_cut[0,:]) < 1 and sum(y_cut[151,:]) < 1 and sum(y_cut[:,0]) < 1 and sum(y_cut[:,151]) < 1:\n","              pass\n","            else:\n","              count_false = count_false + 1\n","              false_crops.append(f'{pt}_z{z}_f{f}_n{n}')\n","\n","          except:\n","            print(f\"file {pt}_z{z}_f{f}_n{n} failed\")\n","            failed_files.append(f'{pt}_z{z}_f{f}_n{n}')\n","            continue\n","\n","  print(f\"{pt_nr+1} out of {len(patient_files_true)} done\")\n","\n","print(f\"{count_false} out of {count_total} falsely cropped\")\n","np.save('/content/drive/MyDrive/Deep Learning Project/false_crop_list3',false_crops)\n","np.save('/content/drive/MyDrive/Deep Learning Project/failed_files_list3',failed_files)"],"metadata":{"id":"3ACCDSK7Vduh"},"execution_count":null,"outputs":[]}]}